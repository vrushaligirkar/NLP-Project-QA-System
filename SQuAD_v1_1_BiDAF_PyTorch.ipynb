{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SQuAD_v1.1_BiDAF_PyTorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO4NmKJ6yW+HT2ccJXSUjUt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrushaligirkar/NLP-Project-QA-system/blob/master/SQuAD_v1_1_BiDAF_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yjg9szkU7ffm",
        "colab_type": "code",
        "outputId": "8d279613-5bc7-4670-dfb8-4989f9197de1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from collections import Counter\n",
        "import string\n",
        "import re\n",
        "import sys\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import copy\n",
        "from datetime import datetime\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "import torchtext\n",
        "from torchtext import data\n",
        "print(torchtext.__version__)\n",
        "from torchtext import datasets\n",
        "from torchtext.vocab import GloVe\n",
        "from torch import optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from google.colab import drive\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "1.4.0\n",
            "0.3.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj0WNQo89Cxf",
        "colab_type": "code",
        "outputId": "a680a5c6-61a3-4d21-c761-967f6445f04b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNj5v8wH9Rre",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Load pickled GTSRB data\n",
        "training_file = '/content/gdrive/My Drive/Colab Notebooks/data_SQuAD_V1.1/train-v1.1.json'\n",
        "dev_file = '/content/gdrive/My Drive/Colab Notebooks/data_SQuAD_V1.1/dev-v1.1.json'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84R2nJ_Zt3xu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = {}\n",
        "args['--char-dim'] = 8\n",
        "args['--char-channel-width'] = 5\n",
        "args['--char-channel-size'] = 100\n",
        "args['--context-threshold'] = 400\n",
        "args['--dev-batch-size'] = 100\n",
        "args['--dev-file'] = dev_file\n",
        "args['--dropout'] = 0.2\n",
        "args['--epoch'] = 12\n",
        "args['--exp-decay-rate'] = 0.999\n",
        "args['--gpu'] = 0 \n",
        "args['--hidden-size'] = 100\n",
        "args['--learning-rate'] = 0.5\n",
        "args['--print-freq'] = 250\n",
        "args['--train-batch-size'] = 60\n",
        "args['--train-file'] = training_file\n",
        "args['--word-dim'] = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0HQzmcJ7sRb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_tokenize(tokens):\n",
        "    return [token.replace(\"''\", '\"').replace(\"``\", '\"') for token in nltk.word_tokenize(tokens)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7ILP0st7uUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SQuAD():\n",
        "    def __init__(self, args):\n",
        "        path = '/content/gdrive/My Drive/Colab Notebooks/data_SQuAD_V1.1'\n",
        "        dataset_path = path + '/torchtext/'\n",
        "        train_examples_path = dataset_path + 'train_examples.pt'\n",
        "        dev_examples_path = dataset_path + 'dev_examples.pt'\n",
        "\n",
        "        print(\"preprocessing data files...\")\n",
        "        self.preprocess_file(args['--train-file'])\n",
        "        self.preprocess_file(args['--dev-file'])\n",
        "        #if not os.path.exists('{}/{}'.format(path, args['--train-file'])):\n",
        "        #    self.preprocess_file('{}/{}'.format(path, args['--train-file']))\n",
        "        #if not os.path.exists('{}/{}l'.format(path, args['--dev-file'])):\n",
        "        #    self.preprocess_file('{}/{}'.format(path, args['--dev-file']))\n",
        "        #if nor os.path.exists(arg)\n",
        "\n",
        "        self.RAW = data.RawField()\n",
        "        # explicit declaration for torchtext compatibility\n",
        "        self.RAW.is_target = False\n",
        "        self.CHAR_NESTING = data.Field(batch_first=True, tokenize=list, lower=True)\n",
        "        self.CHAR = data.NestedField(self.CHAR_NESTING, tokenize=word_tokenize)\n",
        "        self.WORD = data.Field(batch_first=True, tokenize=word_tokenize, lower=True, include_lengths=True)\n",
        "        self.LABEL = data.Field(sequential=False, unk_token=None, use_vocab=False)\n",
        "\n",
        "        dict_fields = {'id': ('id', self.RAW),\n",
        "                       's_idx': ('s_idx', self.LABEL),\n",
        "                       'e_idx': ('e_idx', self.LABEL),\n",
        "                       'context': [('c_word', self.WORD), ('c_char', self.CHAR)],\n",
        "                       'question': [('q_word', self.WORD), ('q_char', self.CHAR)]}\n",
        "\n",
        "        list_fields = [('id', self.RAW), ('s_idx', self.LABEL), ('e_idx', self.LABEL),\n",
        "                       ('c_word', self.WORD), ('c_char', self.CHAR),\n",
        "                       ('q_word', self.WORD), ('q_char', self.CHAR)]\n",
        "\n",
        "        if os.path.exists(dataset_path):\n",
        "            print(\"loading splits...\")\n",
        "            train_examples = torch.load(train_examples_path)\n",
        "            dev_examples = torch.load(dev_examples_path)\n",
        "\n",
        "            self.train = data.Dataset(examples=train_examples, fields=list_fields)\n",
        "            self.dev = data.Dataset(examples=dev_examples, fields=list_fields)\n",
        "        else:\n",
        "            print(\"building splits...\")\n",
        "            self.train, self.dev = data.TabularDataset.splits(\n",
        "                path=path,\n",
        "                train='{}l'.format(args['--train-file']),\n",
        "                validation='{}l'.format(args['--dev-file']),\n",
        "                format='json',\n",
        "                fields=dict_fields)\n",
        "\n",
        "            os.makedirs(dataset_path)\n",
        "            torch.save(self.train.examples, train_examples_path)\n",
        "            torch.save(self.dev.examples, dev_examples_path)\n",
        "\n",
        "        #cut too long context in the training set for efficiency.\n",
        "        if args['--context-threshold'] > 0:\n",
        "            self.train.examples = [e for e in self.train.examples if len(e.c_word) <= args['--context-threshold']]\n",
        "\n",
        "        print(\"building vocab...\")\n",
        "        self.CHAR.build_vocab(self.train, self.dev)\n",
        "        self.WORD.build_vocab(self.train, self.dev, vectors=GloVe(name='6B', dim=args['--word-dim']))\n",
        "\n",
        "        print(\"building iterators...\")\n",
        "        device = torch.device(\"cuda:{}\".format(args['--gpu']) if torch.cuda.is_available() else \"cpu\")\n",
        "        self.train_iter = data.BucketIterator(\n",
        "            self.train,\n",
        "            batch_size=args['--train-batch-size'],\n",
        "            device=device,\n",
        "            repeat=True,\n",
        "            shuffle=True,\n",
        "            sort_key=lambda x: len(x.c_word)\n",
        "        )\n",
        "\n",
        "        self.dev_iter = data.BucketIterator(\n",
        "            self.dev,\n",
        "            batch_size=args['--dev-batch-size'],\n",
        "            device=device,\n",
        "            repeat=False,\n",
        "            sort_key=lambda x: len(x.c_word)\n",
        "        )\n",
        "\n",
        "        # self.train_iter, self.dev_iter = \\\n",
        "        #    data.BucketIterator.splits((self.train, self.dev),\n",
        "        #                               batch_sizes=[args.train_batch_size, args.dev_batch_size],\n",
        "        #                               device=device,\n",
        "        #                               sort_key=lambda x: len(x.c_word))\n",
        "\n",
        "    def preprocess_file(self, path):\n",
        "        dump = []\n",
        "        abnormals = [' ', '\\n', '\\u3000', '\\u202f', '\\u2009']\n",
        "\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            data = data['data']\n",
        "\n",
        "            for article in data:\n",
        "                for paragraph in article['paragraphs']:\n",
        "                    context = paragraph['context']\n",
        "                    tokens = word_tokenize(context)\n",
        "                    for qa in paragraph['qas']:\n",
        "                        id = qa['id']\n",
        "                        question = qa['question']\n",
        "                        for ans in qa['answers']:\n",
        "                            answer = ans['text']\n",
        "                            s_idx = ans['answer_start']\n",
        "                            e_idx = s_idx + len(answer)\n",
        "\n",
        "                            l = 0\n",
        "                            s_found = False\n",
        "                            for i, t in enumerate(tokens):\n",
        "                                while l < len(context):\n",
        "                                    if context[l] in abnormals:\n",
        "                                        l += 1\n",
        "                                    else:\n",
        "                                        break\n",
        "                                # exceptional cases\n",
        "                                if t[0] == '\"' and context[l:l + 2] == '\\'\\'':\n",
        "                                    t = '\\'\\'' + t[1:]\n",
        "                                elif t == '\"' and context[l:l + 2] == '\\'\\'':\n",
        "                                    t = '\\'\\''\n",
        "\n",
        "                                l += len(t)\n",
        "                                if l > s_idx and s_found == False:\n",
        "                                    s_idx = i\n",
        "                                    s_found = True\n",
        "                                if l >= e_idx:\n",
        "                                    e_idx = i\n",
        "                                    break\n",
        "\n",
        "                            dump.append(dict([('id', id),\n",
        "                                              ('context', context),\n",
        "                                              ('question', question),\n",
        "                                              ('answer', answer),\n",
        "                                              ('s_idx', s_idx),\n",
        "                                              ('e_idx', e_idx)]))\n",
        "\n",
        "        with open('{}l'.format(path), 'w', encoding='utf-8') as f:\n",
        "            for line in dump:\n",
        "                json.dump(line, f)\n",
        "                print('', file=f)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0lvwCeC8F4d",
        "colab_type": "code",
        "outputId": "5fa50ab9-db61-42e3-8caf-49610f5130ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "print('loading SQuAD data...')\n",
        "processed_data = SQuAD(args)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading SQuAD data...\n",
            "preprocessing data files...\n",
            "building splits...\n",
            "building vocab...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [06:29, 2.21MB/s]                           \n",
            "100%|█████████▉| 398854/400000 [00:23<00:00, 17244.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "building iterators...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbIJFKrEQ6d9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "13e3dfbf-3a10-4d98-889b-61ffef2a8749"
      },
      "source": [
        "args['char_vocab_size'] = len(processed_data.CHAR.vocab)\n",
        "args['word_vocab_size'] = len(processed_data.WORD.vocab)\n",
        "args['dataset_file'] = args['--dev-file']\n",
        "#args['prediction_file'] = f'prediction{args['--gpu']}.out'\n",
        "args['prediction_file'] = 'prediction0.out'\n",
        "args['model_time'] = datetime.now().strftime('%H:%M:%S')\n",
        "print('data loading complete')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data loading complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgxe_baGgf2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Utils\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, batch_first=False, num_layers=1, bidirectional=False, dropout=0.2):\n",
        "        super(LSTM, self).__init__()\n",
        "\n",
        "        self.rnn = nn.LSTM(input_size=input_size,\n",
        "                           hidden_size=hidden_size,\n",
        "                           num_layers=num_layers,\n",
        "                           bidirectional=bidirectional,\n",
        "                           batch_first=batch_first)\n",
        "        self.reset_params()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "    def reset_params(self):\n",
        "        for i in range(self.rnn.num_layers):\n",
        "            nn.init.orthogonal_(getattr(self.rnn, f'weight_hh_l{i}'))\n",
        "            nn.init.kaiming_normal_(getattr(self.rnn, f'weight_ih_l{i}'))\n",
        "            nn.init.constant_(getattr(self.rnn, f'bias_hh_l{i}'), val=0)\n",
        "            nn.init.constant_(getattr(self.rnn, f'bias_ih_l{i}'), val=0)\n",
        "            getattr(self.rnn, f'bias_hh_l{i}').chunk(4)[1].fill_(1)\n",
        "\n",
        "            if self.rnn.bidirectional:\n",
        "                nn.init.orthogonal_(getattr(self.rnn, f'weight_hh_l{i}_reverse'))\n",
        "                nn.init.kaiming_normal_(getattr(self.rnn, f'weight_ih_l{i}_reverse'))\n",
        "                nn.init.constant_(getattr(self.rnn, f'bias_hh_l{i}_reverse'), val=0)\n",
        "                nn.init.constant_(getattr(self.rnn, f'bias_ih_l{i}_reverse'), val=0)\n",
        "                getattr(self.rnn, f'bias_hh_l{i}_reverse').chunk(4)[1].fill_(1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x, x_len = x\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        x_len_sorted, x_idx = torch.sort(x_len, descending=True)\n",
        "        x_sorted = x.index_select(dim=0, index=x_idx)\n",
        "        _, x_ori_idx = torch.sort(x_idx)\n",
        "\n",
        "        x_packed = nn.utils.rnn.pack_padded_sequence(x_sorted, x_len_sorted, batch_first=True)\n",
        "        x_packed, (h, c) = self.rnn(x_packed)\n",
        "\n",
        "        x = nn.utils.rnn.pad_packed_sequence(x_packed, batch_first=True)[0]\n",
        "        x = x.index_select(dim=0, index=x_ori_idx)\n",
        "        h = h.permute(1, 0, 2).contiguous().view(-1, h.size(0) * h.size(2)).squeeze()\n",
        "        h = h.index_select(dim=0, index=x_ori_idx)\n",
        "\n",
        "        return x, h\n",
        "\n",
        "\n",
        "class Linear(nn.Module):\n",
        "    def __init__(self, in_features, out_features, dropout=0.0):\n",
        "        super(Linear, self).__init__()\n",
        "\n",
        "        self.linear = nn.Linear(in_features=in_features, out_features=out_features)\n",
        "        if dropout > 0:\n",
        "            self.dropout = nn.Dropout(p=dropout)\n",
        "        self.reset_params()\n",
        "\n",
        "    def reset_params(self):\n",
        "        nn.init.kaiming_normal_(self.linear.weight)\n",
        "        nn.init.constant_(self.linear.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if hasattr(self, 'dropout'):\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djbgM7DlXu37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BiDAF(nn.Module):\n",
        "    def __init__(self, args, pretrained):\n",
        "        super(BiDAF, self).__init__()\n",
        "        self.args = args\n",
        "\n",
        "        # 1. Character Embedding Layer\n",
        "        self.char_emb = nn.Embedding(args['char_vocab_size'], args['--char-dim'], padding_idx=1)\n",
        "        nn.init.uniform_(self.char_emb.weight, -0.001, 0.001)\n",
        "\n",
        "        self.char_conv = nn.Sequential(\n",
        "            nn.Conv2d(1, args['--char-channel-size'], (args['--char-dim'], args['--char-channel-width'])),\n",
        "            nn.ReLU()\n",
        "            )\n",
        "\n",
        "        # 2. Word Embedding Layer\n",
        "        # initialize word embedding with GloVe\n",
        "        self.word_emb = nn.Embedding.from_pretrained(pretrained, freeze=True)\n",
        "\n",
        "        # highway network\n",
        "        assert self.args['--hidden-size'] * 2 == (self.args['--char-channel-size'] + args['--word-dim'])\n",
        "        for i in range(2):\n",
        "            setattr(self, 'highway_linear{}'.format(i),\n",
        "                    nn.Sequential(Linear(args['--hidden-size'] * 2, args['--hidden-size'] * 2),\n",
        "                                  nn.ReLU()))\n",
        "            setattr(self, 'highway_gate{}'.format(i),\n",
        "                    nn.Sequential(Linear(args['--hidden-size'] * 2, args['--hidden-size'] * 2),\n",
        "                                  nn.Sigmoid()))\n",
        "\n",
        "        # 3. Contextual Embedding Layer\n",
        "        self.context_LSTM = LSTM(input_size=args['--hidden-size'] * 2,\n",
        "                                 hidden_size=args['--hidden-size'],\n",
        "                                 bidirectional=True,\n",
        "                                 batch_first=True,\n",
        "                                 dropout=args['--dropout'])\n",
        "\n",
        "        # 4. Attention Flow Layer\n",
        "        self.att_weight_c = Linear(args['--hidden-size'] * 2, 1)\n",
        "        self.att_weight_q = Linear(args['--hidden-size'] * 2, 1)\n",
        "        self.att_weight_cq = Linear(args['--hidden-size'] * 2, 1)\n",
        "\n",
        "        # 5. Modeling Layer\n",
        "        self.modeling_LSTM1 = LSTM(input_size=args['--hidden-size'] * 8,\n",
        "                                   hidden_size=args['--hidden-size'],\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=args['--dropout'])\n",
        "\n",
        "        self.modeling_LSTM2 = LSTM(input_size=args['--hidden-size'] * 2,\n",
        "                                   hidden_size=args['--hidden-size'],\n",
        "                                   bidirectional=True,\n",
        "                                   batch_first=True,\n",
        "                                   dropout=args['--dropout'])\n",
        "\n",
        "        # 6. Output Layer\n",
        "        self.p1_weight_g = Linear(args['--hidden-size'] * 8, 1, dropout=args['--dropout'])\n",
        "        self.p1_weight_m = Linear(args['--hidden-size'] * 2, 1, dropout=args['--dropout'])\n",
        "        self.p2_weight_g = Linear(args['--hidden-size'] * 8, 1, dropout=args['--dropout'])\n",
        "        self.p2_weight_m = Linear(args['--hidden-size'] * 2, 1, dropout=args['--dropout'])\n",
        "\n",
        "        self.output_LSTM = LSTM(input_size=args['--hidden-size'] * 2,\n",
        "                                hidden_size=args['--hidden-size'],\n",
        "                                bidirectional=True,\n",
        "                                batch_first=True,\n",
        "                                dropout=args['--dropout'])\n",
        "\n",
        "        self.dropout = nn.Dropout(p=args['--dropout'])\n",
        "\n",
        "    def forward(self, batch):\n",
        "        # TODO: More memory-efficient architecture\n",
        "        def char_emb_layer(x):\n",
        "            \"\"\"\n",
        "            :param x: (batch, seq_len, word_len)\n",
        "            :return: (batch, seq_len, char_channel_size)\n",
        "            \"\"\"\n",
        "            batch_size = x.size(0)\n",
        "            # (batch, seq_len, word_len, char_dim)\n",
        "            x = self.dropout(self.char_emb(x))\n",
        "            # (batch， seq_len, char_dim, word_len)\n",
        "            x = x.transpose(2, 3)\n",
        "            # (batch * seq_len, 1, char_dim, word_len)\n",
        "            x = x.view(-1, self.args['--char-dim'], x.size(3)).unsqueeze(1)\n",
        "            # (batch * seq_len, char_channel_size, 1, conv_len) -> (batch * seq_len, char_channel_size, conv_len)\n",
        "            x = self.char_conv(x).squeeze()\n",
        "            # (batch * seq_len, char_channel_size, 1) -> (batch * seq_len, char_channel_size)\n",
        "            x = F.max_pool1d(x, x.size(2)).squeeze()\n",
        "            # (batch, seq_len, char_channel_size)\n",
        "            x = x.view(batch_size, -1, self.args['--char-channel-size'])\n",
        "\n",
        "            return x\n",
        "\n",
        "        def highway_network(x1, x2):\n",
        "            \"\"\"\n",
        "            :param x1: (batch, seq_len, char_channel_size)\n",
        "            :param x2: (batch, seq_len, word_dim)\n",
        "            :return: (batch, seq_len, hidden_size * 2)\n",
        "            \"\"\"\n",
        "            # (batch, seq_len, char_channel_size + word_dim)\n",
        "            x = torch.cat([x1, x2], dim=-1)\n",
        "            for i in range(2):\n",
        "                h = getattr(self, 'highway_linear{}'.format(i))(x)\n",
        "                g = getattr(self, 'highway_gate{}'.format(i))(x)\n",
        "                x = g * h + (1 - g) * x\n",
        "            # (batch, seq_len, hidden_size * 2)\n",
        "            return x\n",
        "\n",
        "        def att_flow_layer(c, q):\n",
        "            \"\"\"\n",
        "            :param c: (batch, c_len, hidden_size * 2)\n",
        "            :param q: (batch, q_len, hidden_size * 2)\n",
        "            :return: (batch, c_len, q_len)\n",
        "            \"\"\"\n",
        "            c_len = c.size(1)\n",
        "            q_len = q.size(1)\n",
        "\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #c_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #q_tiled = q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "            # (batch, c_len, q_len, hidden_size * 2)\n",
        "            #cq_tiled = c_tiled * q_tiled\n",
        "            #cq_tiled = c.unsqueeze(2).expand(-1, -1, q_len, -1) * q.unsqueeze(1).expand(-1, c_len, -1, -1)\n",
        "\n",
        "            cq = []\n",
        "            for i in range(q_len):\n",
        "                #(batch, 1, hidden_size * 2)\n",
        "                qi = q.select(1, i).unsqueeze(1)\n",
        "                #(batch, c_len, 1)\n",
        "                ci = self.att_weight_cq(c * qi).squeeze()\n",
        "                cq.append(ci)\n",
        "            # (batch, c_len, q_len)\n",
        "            cq = torch.stack(cq, dim=-1)\n",
        "\n",
        "            # (batch, c_len, q_len)\n",
        "            s = self.att_weight_c(c).expand(-1, -1, q_len) + \\\n",
        "                self.att_weight_q(q).permute(0, 2, 1).expand(-1, c_len, -1) + \\\n",
        "                cq\n",
        "\n",
        "            # (batch, c_len, q_len)\n",
        "            a = F.softmax(s, dim=2)\n",
        "            # (batch, c_len, q_len) * (batch, q_len, hidden_size * 2) -> (batch, c_len, hidden_size * 2)\n",
        "            c2q_att = torch.bmm(a, q)\n",
        "            # (batch, 1, c_len)\n",
        "            b = F.softmax(torch.max(s, dim=2)[0], dim=1).unsqueeze(1)\n",
        "            # (batch, 1, c_len) * (batch, c_len, hidden_size * 2) -> (batch, hidden_size * 2)\n",
        "            q2c_att = torch.bmm(b, c).squeeze()\n",
        "            # (batch, c_len, hidden_size * 2) (tiled)\n",
        "            q2c_att = q2c_att.unsqueeze(1).expand(-1, c_len, -1)\n",
        "            # q2c_att = torch.stack([q2c_att] * c_len, dim=1)\n",
        "\n",
        "            # (batch, c_len, hidden_size * 8)\n",
        "            x = torch.cat([c, c2q_att, c * c2q_att, c * q2c_att], dim=-1)\n",
        "            return x\n",
        "\n",
        "        def output_layer(g, m, l):\n",
        "            \"\"\"\n",
        "            :param g: (batch, c_len, hidden_size * 8)\n",
        "            :param m: (batch, c_len ,hidden_size * 2)\n",
        "            :return: p1: (batch, c_len), p2: (batch, c_len)\n",
        "            \"\"\"\n",
        "            # (batch, c_len)\n",
        "            p1 = (self.p1_weight_g(g) + self.p1_weight_m(m)).squeeze()\n",
        "            # (batch, c_len, hidden_size * 2)\n",
        "            m2 = self.output_LSTM((m, l))[0]\n",
        "            # (batch, c_len)\n",
        "            p2 = (self.p2_weight_g(g) + self.p2_weight_m(m2)).squeeze()\n",
        "\n",
        "            return p1, p2\n",
        "\n",
        "        # 1. Character Embedding Layer\n",
        "        c_char = char_emb_layer(batch.c_char)\n",
        "        q_char = char_emb_layer(batch.q_char)\n",
        "        # 2. Word Embedding Layer\n",
        "        c_word = self.word_emb(batch.c_word[0])\n",
        "        q_word = self.word_emb(batch.q_word[0])\n",
        "        c_lens = batch.c_word[1]\n",
        "        q_lens = batch.q_word[1]\n",
        "\n",
        "        # Highway network\n",
        "        c = highway_network(c_char, c_word)\n",
        "        q = highway_network(q_char, q_word)\n",
        "        # 3. Contextual Embedding Layer\n",
        "        c = self.context_LSTM((c, c_lens))[0]\n",
        "        q = self.context_LSTM((q, q_lens))[0]\n",
        "        # 4. Attention Flow Layer\n",
        "        g = att_flow_layer(c, q)\n",
        "        # 5. Modeling Layer\n",
        "        m = self.modeling_LSTM2((self.modeling_LSTM1((g, c_lens))[0], c_lens))[0]\n",
        "        # 6. Output Layer\n",
        "        p1, p2 = output_layer(g, m, c_lens)\n",
        "\n",
        "        # (batch, c_len), (batch, c_len)\n",
        "        return p1, p2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xj7itLTReFsf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class EMA():\n",
        "    def __init__(self, mu):\n",
        "        self.mu = mu\n",
        "        self.shadow = {}\n",
        "\n",
        "    def register(self, name, val):\n",
        "        self.shadow[name] = val.clone()\n",
        "\n",
        "    def get(self, name):\n",
        "        return self.shadow[name]\n",
        "\n",
        "    def update(self, name, x):\n",
        "        assert name in self.shadow\n",
        "        new_average = (1.0 - self.mu) * x + self.mu * self.shadow[name]\n",
        "        self.shadow[name] = new_average.clone()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIAYGgbmWvAH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(args, data):\n",
        "    device = torch.device(f\"cuda:{args['--gpu']}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = BiDAF(args, data.WORD.vocab.vectors).to(device)\n",
        "\n",
        "    ema = EMA(args['--exp-decay-rate'])\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            ema.register(name, param.data)\n",
        "    parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
        "    optimizer = optim.Adadelta(parameters, lr=args['--learning-rate'])\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    writer = SummaryWriter(log_dir='runs/' + args['model_time'])\n",
        "\n",
        "    model.train()\n",
        "    loss, last_epoch = 0, -1\n",
        "    max_dev_exact, max_dev_f1 = -1, -1\n",
        "\n",
        "    iterator = data.train_iter\n",
        "    for i, batch in enumerate(iterator):\n",
        "        present_epoch = int(iterator.epoch)\n",
        "        if present_epoch == args['--epoch']:\n",
        "            break\n",
        "        if present_epoch > last_epoch:\n",
        "            print('epoch:', present_epoch + 1)\n",
        "        last_epoch = present_epoch\n",
        "\n",
        "        p1, p2 = model(batch)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "        loss += batch_loss.item()\n",
        "        batch_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                ema.update(name, param.data)\n",
        "\n",
        "        if (i + 1) % args['--print-freq'] == 0:\n",
        "            dev_loss, dev_exact, dev_f1 = test(model, ema, args, data)\n",
        "            c = (i + 1) // args['--print-freq']\n",
        "\n",
        "            writer.add_scalar('loss/train', loss, c)\n",
        "            writer.add_scalar('loss/dev', dev_loss, c)\n",
        "            writer.add_scalar('exact_match/dev', dev_exact, c)\n",
        "            writer.add_scalar('f1/dev', dev_f1, c)\n",
        "            print(f'train loss: {loss:.3f} / dev loss: {dev_loss:.3f}'\n",
        "                  f' / dev EM: {dev_exact:.3f} / dev F1: {dev_f1:.3f}')\n",
        "\n",
        "            if dev_f1 > max_dev_f1:\n",
        "                max_dev_f1 = dev_f1\n",
        "                max_dev_exact = dev_exact\n",
        "                best_model = copy.deepcopy(model)\n",
        "\n",
        "            loss = 0\n",
        "            model.train()\n",
        "\n",
        "    writer.close()\n",
        "    print(f'max dev EM: {max_dev_exact:.3f} / max dev F1: {max_dev_f1:.3f}')\n",
        "\n",
        "    return best_model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAgRsYtqjz7z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, ema, args, data):\n",
        "    device = torch.device(f\"cuda:{args['--gpu']}\" if torch.cuda.is_available() else \"cpu\")\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    loss = 0\n",
        "    answers = dict()\n",
        "    model.eval()\n",
        "\n",
        "    backup_params = EMA(0)\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.requires_grad:\n",
        "            backup_params.register(name, param.data)\n",
        "            param.data.copy_(ema.get(name))\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        for batch in iter(data.dev_iter):\n",
        "            p1, p2 = model(batch)\n",
        "            batch_loss = criterion(p1, batch.s_idx) + criterion(p2, batch.e_idx)\n",
        "            loss += batch_loss.item()\n",
        "\n",
        "            # (batch, c_len, c_len)\n",
        "            batch_size, c_len = p1.size()\n",
        "            ls = nn.LogSoftmax(dim=1)\n",
        "            mask = (torch.ones(c_len, c_len) * float('-inf')).to(device).tril(-1).unsqueeze(0).expand(batch_size, -1, -1)\n",
        "            score = (ls(p1).unsqueeze(2) + ls(p2).unsqueeze(1)) + mask\n",
        "            score, s_idx = score.max(dim=1)\n",
        "            score, e_idx = score.max(dim=1)\n",
        "            s_idx = torch.gather(s_idx, 1, e_idx.view(-1, 1)).squeeze()\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                id = batch.id[i]\n",
        "                answer = batch.c_word[0][i][s_idx[i]:e_idx[i]+1]\n",
        "                answer = ' '.join([data.WORD.vocab.itos[idx] for idx in answer])\n",
        "                answers[id] = answer\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                param.data.copy_(backup_params.get(name))\n",
        "\n",
        "    with open(args['prediction_file'], 'w', encoding='utf-8') as f:\n",
        "        print(json.dumps(answers), file=f)\n",
        "\n",
        "    results = evaluate_main(args)\n",
        "    return loss, results['exact_match'], results['f1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MV9hwexmlHAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return ' '.join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return ''.join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "\n",
        "def exact_match_score(prediction, ground_truth):\n",
        "    return (normalize_answer(prediction) == normalize_answer(ground_truth))\n",
        "\n",
        "\n",
        "def metric_max_over_ground_truths(metric_fn, prediction, ground_truths):\n",
        "    scores_for_ground_truths = []\n",
        "    for ground_truth in ground_truths:\n",
        "        score = metric_fn(prediction, ground_truth)\n",
        "        scores_for_ground_truths.append(score)\n",
        "    return max(scores_for_ground_truths)\n",
        "\n",
        "\n",
        "def evaluate(dataset, predictions):\n",
        "    f1 = exact_match = total = 0\n",
        "    for article in dataset:\n",
        "        for paragraph in article['paragraphs']:\n",
        "            for qa in paragraph['qas']:\n",
        "                total += 1\n",
        "                if qa['id'] not in predictions:\n",
        "                    message = 'Unanswered question ' + qa['id'] + \\\n",
        "                              ' will receive score 0.'\n",
        "                    print(message, file=sys.stderr)\n",
        "                    continue\n",
        "                ground_truths = list(map(lambda x: x['text'], qa['answers']))\n",
        "                prediction = predictions[qa['id']]\n",
        "                exact_match += metric_max_over_ground_truths(\n",
        "                    exact_match_score, prediction, ground_truths)\n",
        "                f1 += metric_max_over_ground_truths(\n",
        "                    f1_score, prediction, ground_truths)\n",
        "\n",
        "    exact_match = 100.0 * exact_match / total\n",
        "    f1 = 100.0 * f1 / total\n",
        "\n",
        "    return {'exact_match': exact_match, 'f1': f1}\n",
        "\n",
        "\n",
        "def evaluate_main(args):\n",
        "    with open(args['dataset_file']) as dataset_file:\n",
        "        dataset_json = json.load(dataset_file)\n",
        "        \"\"\"\n",
        "        if (dataset_json['version'] != expected_version):\n",
        "            print('Evaluation expects v-' + expected_version +\n",
        "                  ', but got dataset with v-' + dataset_json['version'],\n",
        "                  file=sys.stderr)\n",
        "        \"\"\"\n",
        "        dataset = dataset_json['data']\n",
        "    with open(args['prediction_file']) as prediction_file:\n",
        "        predictions = json.load(prediction_file)\n",
        "\n",
        "    results = evaluate(dataset, predictions)\n",
        "    #print(json.dumps(results))\n",
        "    return results\n",
        "\n",
        "\n",
        "#if __name__ == '__main__':\n",
        "#    expected_version = '1.1'\n",
        "#    parser = argparse.ArgumentParser(\n",
        "#        description='Evaluation for SQuAD ' + expected_version)\n",
        "#    parser.add_argument('dataset_file', help='Dataset file')\n",
        "#    parser.add_argument('prediction_file', help='Prediction File')\n",
        "#    args = parser.parse_args()\n",
        "#    main(args)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1NhG4cwWbB5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c0867cb-b3a8-4201-af2d-30ffa7da3167"
      },
      "source": [
        "print('training start!')\n",
        "best_model = train(args, processed_data)\n",
        "if not os.path.exists('/content/gdrive/My Drive/Colab Notebooks/data_SQuAD_V1.1/saved_models'):\n",
        "        os.makedirs('/content/gdrive/My Drive/Colab Notebooks/data_SQuAD_V1.1/saved_models')\n",
        "torch.save(best_model.state_dict(), '/content/gdrive/My Drive/Colab Notebooks/data_SQuAD_V1.1/saved_models/BiDAF_'+ args['model_time'] +'.pt')\n",
        "print('training finished!')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training start!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|█████████▉| 398854/400000 [00:40<00:00, 17244.88it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch: 1\n",
            "train loss: 2070.772 / dev loss: 3539.240 / dev EM: 1.230 / dev F1: 9.126\n",
            "train loss: 1850.599 / dev loss: 3274.463 / dev EM: 3.633 / dev F1: 11.073\n",
            "train loss: 1657.991 / dev loss: 3073.673 / dev EM: 4.910 / dev F1: 12.096\n",
            "train loss: 1577.429 / dev loss: 2889.399 / dev EM: 6.244 / dev F1: 13.866\n",
            "train loss: 1456.331 / dev loss: 2691.074 / dev EM: 10.076 / dev F1: 18.701\n",
            "epoch: 2\n",
            "train loss: 1295.745 / dev loss: 2476.353 / dev EM: 14.702 / dev F1: 24.602\n",
            "train loss: 1149.287 / dev loss: 2286.276 / dev EM: 19.555 / dev F1: 30.377\n",
            "train loss: 1078.824 / dev loss: 2122.032 / dev EM: 24.324 / dev F1: 36.413\n",
            "train loss: 1036.599 / dev loss: 1939.200 / dev EM: 29.224 / dev F1: 42.759\n",
            "train loss: 999.008 / dev loss: 1731.036 / dev EM: 35.818 / dev F1: 50.072\n",
            "train loss: 959.431 / dev loss: 1547.632 / dev EM: 42.119 / dev F1: 56.336\n",
            "epoch: 3\n",
            "train loss: 940.528 / dev loss: 1416.636 / dev EM: 46.017 / dev F1: 60.245\n",
            "train loss: 893.946 / dev loss: 1333.988 / dev EM: 48.827 / dev F1: 62.729\n",
            "train loss: 889.071 / dev loss: 1280.403 / dev EM: 50.861 / dev F1: 64.443\n",
            "train loss: 882.931 / dev loss: 1244.725 / dev EM: 52.157 / dev F1: 65.538\n",
            "train loss: 874.751 / dev loss: 1218.027 / dev EM: 53.652 / dev F1: 66.652\n",
            "train loss: 879.528 / dev loss: 1197.552 / dev EM: 54.551 / dev F1: 67.393\n",
            "epoch: 4\n",
            "train loss: 840.559 / dev loss: 1180.551 / dev EM: 55.232 / dev F1: 67.986\n",
            "train loss: 818.466 / dev loss: 1167.956 / dev EM: 55.885 / dev F1: 68.417\n",
            "train loss: 821.842 / dev loss: 1156.180 / dev EM: 56.310 / dev F1: 68.906\n",
            "train loss: 822.758 / dev loss: 1147.125 / dev EM: 57.058 / dev F1: 69.475\n",
            "train loss: 810.795 / dev loss: 1138.755 / dev EM: 57.351 / dev F1: 69.661\n",
            "train loss: 801.573 / dev loss: 1129.694 / dev EM: 58.202 / dev F1: 70.218\n",
            "epoch: 5\n",
            "train loss: 770.363 / dev loss: 1123.302 / dev EM: 58.675 / dev F1: 70.607\n",
            "train loss: 765.775 / dev loss: 1117.874 / dev EM: 59.016 / dev F1: 70.958\n",
            "train loss: 769.440 / dev loss: 1112.511 / dev EM: 59.612 / dev F1: 71.406\n",
            "train loss: 765.897 / dev loss: 1106.961 / dev EM: 59.773 / dev F1: 71.637\n",
            "train loss: 766.659 / dev loss: 1101.279 / dev EM: 60.218 / dev F1: 72.036\n",
            "train loss: 769.720 / dev loss: 1096.713 / dev EM: 60.520 / dev F1: 72.331\n",
            "epoch: 6\n",
            "train loss: 719.850 / dev loss: 1092.716 / dev EM: 60.728 / dev F1: 72.526\n",
            "train loss: 730.841 / dev loss: 1089.033 / dev EM: 61.069 / dev F1: 72.662\n",
            "train loss: 736.369 / dev loss: 1085.990 / dev EM: 61.315 / dev F1: 72.822\n",
            "train loss: 727.234 / dev loss: 1081.428 / dev EM: 61.693 / dev F1: 73.202\n",
            "train loss: 728.993 / dev loss: 1078.394 / dev EM: 61.637 / dev F1: 73.143\n",
            "epoch: 7\n",
            "train loss: 731.033 / dev loss: 1074.805 / dev EM: 61.760 / dev F1: 73.300\n",
            "train loss: 689.088 / dev loss: 1072.190 / dev EM: 61.977 / dev F1: 73.399\n",
            "train loss: 695.529 / dev loss: 1070.326 / dev EM: 62.015 / dev F1: 73.469\n",
            "train loss: 702.523 / dev loss: 1069.905 / dev EM: 62.242 / dev F1: 73.654\n",
            "train loss: 705.642 / dev loss: 1065.860 / dev EM: 62.498 / dev F1: 73.846\n",
            "train loss: 698.521 / dev loss: 1062.931 / dev EM: 62.772 / dev F1: 74.021\n",
            "epoch: 8\n",
            "train loss: 689.492 / dev loss: 1061.810 / dev EM: 62.838 / dev F1: 74.069\n",
            "train loss: 664.534 / dev loss: 1061.405 / dev EM: 62.857 / dev F1: 74.102\n",
            "train loss: 661.563 / dev loss: 1060.665 / dev EM: 63.009 / dev F1: 74.311\n",
            "train loss: 673.502 / dev loss: 1059.458 / dev EM: 62.923 / dev F1: 74.285\n",
            "train loss: 673.193 / dev loss: 1057.915 / dev EM: 63.084 / dev F1: 74.352\n",
            "train loss: 686.373 / dev loss: 1056.543 / dev EM: 63.198 / dev F1: 74.406\n",
            "epoch: 9\n",
            "train loss: 665.492 / dev loss: 1054.858 / dev EM: 63.349 / dev F1: 74.454\n",
            "train loss: 633.340 / dev loss: 1055.447 / dev EM: 63.349 / dev F1: 74.464\n",
            "train loss: 653.318 / dev loss: 1054.496 / dev EM: 63.491 / dev F1: 74.647\n",
            "train loss: 650.284 / dev loss: 1052.271 / dev EM: 63.576 / dev F1: 74.745\n",
            "train loss: 657.473 / dev loss: 1051.076 / dev EM: 63.586 / dev F1: 74.736\n",
            "train loss: 645.699 / dev loss: 1052.050 / dev EM: 63.671 / dev F1: 74.809\n",
            "epoch: 10\n",
            "train loss: 629.859 / dev loss: 1050.509 / dev EM: 63.794 / dev F1: 74.878\n",
            "train loss: 618.966 / dev loss: 1050.669 / dev EM: 63.623 / dev F1: 74.799\n",
            "train loss: 622.432 / dev loss: 1050.926 / dev EM: 63.680 / dev F1: 74.875\n",
            "train loss: 637.273 / dev loss: 1050.607 / dev EM: 63.888 / dev F1: 74.911\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}