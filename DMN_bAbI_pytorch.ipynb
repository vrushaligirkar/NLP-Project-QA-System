{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DMN_bAbI_pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN1RVgfd7zrfenFfhH0Fdla",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vrushaligirkar/NLP-Project-QA-system/blob/master/DMN_bAbI_pytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFbe2KKqeycn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "outputId": "5d8d9a1d-45ce-4cbd-c5f6-faad5325a687"
      },
      "source": [
        "#REF: https://github.com/jhyuklee/dmn-pytorch\n",
        "import os\n",
        "import numpy as np\n",
        "import sys\n",
        "import string\n",
        "import pprint\n",
        "import copy\n",
        "import pickle\n",
        "import math\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from datetime import datetime\n",
        "\n",
        "!pip install tensorboardX\n",
        "from tensorboardX import SummaryWriter\n",
        "\n",
        "from google.colab import drive"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (46.1.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LWG-UmNnr8u",
        "colab_type": "code",
        "outputId": "1a517464-be78-418f-8286-8a6a36b77ce9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO7VnCPSpKwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dataset(object):\n",
        "    def __init__(self, config):\n",
        "        self.config = config\n",
        "        self.init_settings()\n",
        "        self.init_dict()\n",
        "        self.build_word_dict(self.config.data_dir)\n",
        "        self.get_pretrained_word(self.config.word2vec_path)\n",
        "        self.process_data(self.config.data_dir)\n",
        "\n",
        "    def init_settings(self):\n",
        "        self.dataset = {}\n",
        "        self.train_ptr = 0\n",
        "        self.valid_ptr = 0\n",
        "        self.test_ptr = 0\n",
        "\n",
        "    def init_dict(self):\n",
        "        self.PAD = 'PAD'\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx2vec = []  # pretrained\n",
        "        self.word2idx[self.PAD] = 0\n",
        "        self.idx2word[0] = self.PAD\n",
        "        self.init_word_dict = {}\n",
        "    \n",
        "    def update_word_dict(self, key):\n",
        "        if key not in self.word2idx:\n",
        "            self.word2idx[key] = len(self.word2idx)\n",
        "            self.idx2word[len(self.idx2word)] = key\n",
        "\n",
        "    def map_dict(self, key_list, dictionary):\n",
        "        output = []\n",
        "        for key in key_list:\n",
        "            assert key in dictionary\n",
        "            if key in dictionary:\n",
        "                output.append(dictionary[key])\n",
        "        return output\n",
        "    \n",
        "    def build_word_dict(self, dir):\n",
        "        print('### building word dict %s' % dir)\n",
        "        for subdir, _, files, in os.walk(dir):\n",
        "            print('num of files:',len(sorted(files)))\n",
        "            for file in sorted(files):\n",
        "                with open(os.path.join(subdir, file)) as f:\n",
        "                    for line_idx, line in enumerate(f):\n",
        "                        line = line[:-1]\n",
        "                        story_idx = int(line.split(' ')[0])\n",
        "\n",
        "                        def update_init_dict(split):\n",
        "                            for word in split:\n",
        "                                if word not in self.init_word_dict:\n",
        "                                    self.init_word_dict[word] = (\n",
        "                                            len(self.init_word_dict), 1)\n",
        "                                else:\n",
        "                                    self.init_word_dict[word] = (\n",
        "                                            self.init_word_dict[word][0],\n",
        "                                            self.init_word_dict[word][1] + 1)\n",
        "\n",
        "                        if '\\t' in line: # question\n",
        "                            question, answer, _ = line.split('\\t')\n",
        "                            question = ' '.join(question.split(' ')[1:])\n",
        "                            q_split = nltk.word_tokenize(question)\n",
        "                            if self.config.word2vec_type == 6:\n",
        "                                q_split = [w.lower() for w in q_split]\n",
        "                            update_init_dict(q_split)\n",
        "\n",
        "                            answer = answer.split(',') if ',' in answer else [answer]\n",
        "                            if self.config.word2vec_type == 6:\n",
        "                                answer = [w.lower() for w in answer]\n",
        "                            update_init_dict(answer)\n",
        "                            # TODO: check vocab\n",
        "                            \"\"\"\n",
        "                            for a in answer: \n",
        "                                if a not in self.init_word_dict:\n",
        "                                    print(a)\n",
        "                            \"\"\"\n",
        "                        else: # story\n",
        "                            story_line = ' '.join(line.split(' ')[1:])\n",
        "                            s_split = nltk.word_tokenize(story_line)\n",
        "                            if self.config.word2vec_type == 6:\n",
        "                                s_split = [w.lower() for w in s_split]\n",
        "                            update_init_dict(s_split)\n",
        "\n",
        "        print('init dict size', len(self.init_word_dict))\n",
        "        # print(self.init_word_dict)\n",
        "\n",
        "    def get_pretrained_word(self, path):\n",
        "        print('\\n### loading pretrained %s' % path)\n",
        "        word2vec = {}\n",
        "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "            while True:\n",
        "                try:\n",
        "                    line = f.readline()\n",
        "                    if not line: break\n",
        "                    word = line.split()[0]\n",
        "                    vec = [float(l) for l in line.split()[1:]]\n",
        "                    word2vec[word] = vec\n",
        "                except ValueError as e:\n",
        "                    print(e)\n",
        "        \n",
        "        unk_cnt = 0\n",
        "        self.idx2vec.append([0.0] * self.config.word_embed_dim) # PAD\n",
        "\n",
        "        for word, (word_idx, word_cnt) in self.init_word_dict.items():\n",
        "            if word != 'UNK' and word !='PAD':\n",
        "                assert word_cnt > 0\n",
        "                if word in word2vec:\n",
        "                    self.update_word_dict(word)\n",
        "                    self.idx2vec.append(word2vec[word])\n",
        "                else:\n",
        "                    unk_cnt += 1\n",
        "        print('len(word2idx):',len(self.word2idx))\n",
        "        print('len(word2vec):',len(word2vec['apple'][:5]))\n",
        "        print('apple:', self.word2idx['apple'], word2vec['apple'][:5])\n",
        "        print('apple:', self.idx2vec[self.word2idx['apple']][:5])\n",
        "        print('pretrained vectors', np.asarray(self.idx2vec).shape, 'unk', unk_cnt)\n",
        "        print('dictionary change', len(self.init_word_dict), \n",
        "                'to', len(self.word2idx), len(self.idx2word))\n",
        "\n",
        "    def process_data(self, dir):\n",
        "        print('\\n### processing %s' % dir)\n",
        "        for subdir, _, files, in os.walk(dir):\n",
        "            for file in sorted(files):\n",
        "                with open(os.path.join(subdir, file)) as f:\n",
        "                    max_sentnum = max_slen = max_qlen = 0\n",
        "                    qa_num = file.split('_')[0][2:]\n",
        "                    set_type = file.split('_')[-1][:-4]\n",
        "                    story_list = []\n",
        "                    sf_cnt = 1\n",
        "                    si2sf = {}\n",
        "                    total_data = []\n",
        "\n",
        "                    for line_idx, line in enumerate(f):\n",
        "                        line = line[:-1]\n",
        "                        story_idx = int(line.split(' ')[0])\n",
        "                        if story_idx == 1: \n",
        "                            story_list = []\n",
        "                            sf_cnt = 1\n",
        "                            si2sf = {}\n",
        "\n",
        "                        if '\\t' in line: # question\n",
        "                            question, answer, sup_fact = line.split('\\t')\n",
        "                            question = ' '.join(question.split(' ')[1:])\n",
        "                            q_split = nltk.word_tokenize(question)\n",
        "                            if self.config.word2vec_type == 6:\n",
        "                                q_split = [w.lower() for w in q_split]\n",
        "                            q_split = self.map_dict(q_split, self.word2idx)\n",
        "\n",
        "                            answer = answer.split(',') if ',' in answer else [answer]\n",
        "                            if self.config.word2vec_type == 6:\n",
        "                                answer = [w.lower() for w in answer]\n",
        "                            answer = self.map_dict(answer, self.word2idx)\n",
        "                            sup_fact = [si2sf[int(sf)] for sf in sup_fact.split()]\n",
        "\n",
        "                            sentnum = story_list.count(self.word2idx['.'])\n",
        "                            max_sentnum = max_sentnum if max_sentnum > sentnum \\\n",
        "                                    else sentnum\n",
        "                            max_slen = max_slen if max_slen > len(story_list) \\\n",
        "                                    else len(story_list)\n",
        "                            max_qlen = max_qlen if max_qlen > len(q_split) \\\n",
        "                                    else len(q_split)\n",
        "\n",
        "                            story_tmp = story_list[:]\n",
        "                            total_data.append([story_tmp, q_split, answer, sup_fact])\n",
        "\n",
        "                        else: # story\n",
        "                            story_line = ' '.join(line.split(' ')[1:])\n",
        "                            s_split = nltk.word_tokenize(story_line)\n",
        "                            if self.config.word2vec_type == 6:\n",
        "                                s_split = [w.lower() for w in s_split]\n",
        "                            s_split = self.map_dict(s_split, self.word2idx)\n",
        "                            story_list += s_split\n",
        "                            si2sf[story_idx] = sf_cnt\n",
        "                            sf_cnt += 1\n",
        "\n",
        "                    self.dataset[str(qa_num) + '_' + set_type] = total_data\n",
        "                    def check_update(d, k, v):\n",
        "                        if k in d:\n",
        "                            d[k] = v if v > d[k] else d[k]\n",
        "                        else:\n",
        "                            d[k] = v\n",
        "                    check_update(self.config.max_sentnum, int(qa_num), max_sentnum)\n",
        "                    check_update(self.config.max_slen, int(qa_num), max_slen)\n",
        "                    check_update(self.config.max_qlen, int(qa_num), max_qlen)\n",
        "                    self.config.word_vocab_size = len(self.word2idx)\n",
        "\n",
        "        print('data size', len(total_data))\n",
        "        print('max sentnum', max_sentnum)\n",
        "        print('max slen', max_slen)\n",
        "        print('max qlen', max_qlen, end='\\n\\n')\n",
        "\n",
        "    def pad_sent_word(self, sentword, maxlen):\n",
        "        while len(sentword) != maxlen:\n",
        "            sentword.append(self.word2idx[self.PAD])\n",
        "\n",
        "    def pad_data(self, dataset, set_num):\n",
        "        for data in dataset:\n",
        "            story, question, _, _ = data\n",
        "            self.pad_sent_word(story, self.config.max_slen[set_num])\n",
        "            self.pad_sent_word(question, self.config.max_qlen[set_num])\n",
        "\n",
        "        return dataset\n",
        "    \n",
        "    def get_next_batch(self, mode='tr', set_num=1, batch_size=None):\n",
        "        if batch_size is None:\n",
        "            batch_size = self.config.batch_size\n",
        "        \n",
        "        if mode == 'tr':\n",
        "            ptr = self.train_ptr\n",
        "            data = self.dataset[str(set_num) + '_train']\n",
        "        elif mode == 'va':\n",
        "            ptr = self.valid_ptr\n",
        "            data = self.dataset[str(set_num) + '_valid']\n",
        "        elif mode == 'te':\n",
        "            ptr = self.test_ptr\n",
        "            data = self.dataset[str(set_num) + '_test']\n",
        "        \n",
        "        batch_size = (batch_size if ptr+batch_size<=len(data) else len(data)-ptr)\n",
        "        padded_data = self.pad_data(copy.deepcopy(data[ptr:ptr+batch_size]), set_num)\n",
        "        stories = [d[0] for d in padded_data]\n",
        "        questions = [d[1] for d in padded_data]\n",
        "        answers = [d[2] for d in padded_data]\n",
        "        if len(np.array(answers).shape) < 2:\n",
        "            for answer in answers:\n",
        "                while len(answer) != self.config.max_alen:\n",
        "                    answer.append(-100)\n",
        "        sup_facts = [d[3] for d in padded_data]\n",
        "        for sup_fact in sup_facts:\n",
        "            while len(sup_fact) < self.config.max_episode:\n",
        "                sup_fact.append(self.config.max_sentnum[set_num]+1)\n",
        "        s_lengths = [[idx+1 for idx, val in enumerate(d[0]) \n",
        "            if val == self.word2idx['.']] for d in padded_data]\n",
        "        e_lengths = []\n",
        "        for s_len in s_lengths:\n",
        "            e_lengths.append(len(s_len))\n",
        "            while len(s_len) != self.config.max_sentnum[set_num]:\n",
        "                s_len.append(0)\n",
        "        q_lengths = [[idx+1 for idx, val in enumerate(d[1]) \n",
        "            if val == self.word2idx['?']][0] for d in padded_data]\n",
        "        \n",
        "        if mode == 'tr':\n",
        "            self.train_ptr = (ptr + batch_size) % len(data)\n",
        "        elif mode == 'va':\n",
        "            self.valid_ptr = (ptr + batch_size) % len(data)\n",
        "        elif mode == 'te':\n",
        "            self.test_ptr = (ptr + batch_size) % len(data)\n",
        "\n",
        "        return (stories, questions, answers, sup_facts, \n",
        "                s_lengths, q_lengths, e_lengths)\n",
        "    \n",
        "    def get_batch_ptr(self, mode):\n",
        "        if mode == 'tr':\n",
        "            return self.train_ptr\n",
        "        elif mode == 'va':\n",
        "            return self.valid_ptr\n",
        "        elif mode == 'te':\n",
        "            return self.test_ptr\n",
        "\n",
        "    def get_dataset_len(self, mode, set_num):\n",
        "        if mode == 'tr':\n",
        "            return len(self.dataset[str(set_num) + '_train'])\n",
        "        elif mode == 'va':\n",
        "            return len(self.dataset[str(set_num) + '_valid'])\n",
        "        elif mode == 'te':\n",
        "            return len(self.dataset[str(set_num) + '_test'])\n",
        "\n",
        "    def init_batch_ptr(self, mode=None):\n",
        "        if mode is None:\n",
        "            self.train_ptr = 0\n",
        "            self.valid_ptr = 0\n",
        "            self.test_ptr = 0\n",
        "        elif mode == 'tr':\n",
        "            self.train_ptr = 0\n",
        "        elif mode == 'va':\n",
        "            self.valid_ptr = 0\n",
        "        elif mode == 'te':\n",
        "            self.test_ptr = 0\n",
        "\n",
        "    def shuffle_data(self, mode='tr', set_num=1, seed=None):\n",
        "        if seed is not None:\n",
        "            np.random.seed(seed)\n",
        "        if mode == 'tr':\n",
        "            np.random.shuffle(self.dataset[str(set_num) + '_train'])\n",
        "        elif mode == 'va':\n",
        "            np.random.shuffle(self.dataset[str(set_num) + '_train'])\n",
        "        elif mode == 'te':\n",
        "            np.random.shuffle(self.dataset[str(set_num) + '_test'])\n",
        "\n",
        "    def decode_data(self, s, q, a, sf, l):\n",
        "        print(l)\n",
        "        print('story:', \n",
        "                ' '.join(self.map_dict(s[:l[-1]], self.idx2word)))\n",
        "        print('question:', ' '.join(self.map_dict(q, self.idx2word)))\n",
        "        print('answer:', self.map_dict(a, self.idx2word))\n",
        "        print('supporting fact:', sf)\n",
        "        print('length of sentences:', l)\n",
        "\n",
        "    \n",
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        self.path = '/content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch'\n",
        "        self.data_dir = self.path + '/data/bAbI/en'\n",
        "        self.word2vec_type = 6  # 6 or 840 (B)\n",
        "        self.word2vec_path = self.path + '/data/glove/glove.'\\\n",
        "                + str(self.word2vec_type) + 'B.300d.txt'\n",
        "        self.word_embed_dim = 300\n",
        "        self.batch_size = 32\n",
        "        self.max_sentnum = {}\n",
        "        self.max_slen = {}\n",
        "        self.max_qlen = {}\n",
        "        self.max_episode = 5\n",
        "        self.word_vocab_size = 0\n",
        "        self.save_preprocess = True\n",
        "        self.preprocess_save_path = self.path + '/data/bAbI/babi(tmp).pkl'\n",
        "        self.preprocess_load_path = self.path + '/data/bAbI/babi(10k).pkl'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYhTZF1hsNQY",
        "colab_type": "code",
        "outputId": "f7d55e26-ad19-4d3a-a19b-842c59cc63a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "config = Config()\n",
        "if config.save_preprocess:\n",
        "  dataset = Dataset(config)\n",
        "  pickle.dump(dataset, open(config.preprocess_save_path, 'wb'))\n",
        "else:\n",
        "  print('## load preprocess %s' % config.preprocess_load_path)\n",
        "  dataset = pickle.load(open(config.preprocess_load_path, 'rb'))\n",
        "   \n",
        "# dataset config must be valid\n",
        "pp = lambda x: pprint.PrettyPrinter().pprint(x)\n",
        "pp(([(k,v) for k, v in vars(dataset.config).items() if '__' not in k]))\n",
        "print()\n",
        "   \n",
        "for set_num in range(1):\n",
        "  \"\"\"\n",
        "        mode = 'tr'\n",
        "        while True:\n",
        "            i, t, l = dataset.get_next_batch(mode, set_num+1, batch_size=1000)\n",
        "            print(dataset.get_batch_ptr(mode), len(i))\n",
        "            if dataset.get_batch_ptr(mode) == 0:\n",
        "                print('iteration test pass!', mode)\n",
        "                break\n",
        "        mode = 'va'\n",
        "        while True:\n",
        "            i, t, l = dataset.get_next_batch(mode, set_num+1, batch_size=100)\n",
        "            print(dataset.get_batch_ptr(mode), len(i))\n",
        "            if dataset.get_batch_ptr(mode) == 0:\n",
        "                print('iteration test pass!', mode)\n",
        "                break\n",
        "        \"\"\"\n",
        "  mode = 'te'\n",
        "  dataset.shuffle_data(mode, set_num+1)\n",
        "  while True:\n",
        "    s, q, a, sf, sl, ql, el = dataset.get_next_batch(mode, set_num+1, batch_size=100)\n",
        "    print(dataset.get_batch_ptr(mode), len(s))\n",
        "    if dataset.get_batch_ptr(mode) == 0:\n",
        "      print(s[0], q[0], a[0], sf[0], sl[0], ql[0], el[0])\n",
        "      dataset.decode_data(s[0], q[0], a[0], sf[0], sl[0][:el[0]])\n",
        "      print('iteration test pass!', mode)\n",
        "      break"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "### building word dict /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/data/bAbI/en\n",
            "num of files: 60\n",
            "init dict size 158\n",
            "\n",
            "### loading pretrained /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/data/glove/glove.6B.300d.txt\n",
            "len(word2idx): 159\n",
            "len(word2vec): 5\n",
            "apple: 131 [-0.20842, -0.019668, 0.063981, -0.71403, -0.21181]\n",
            "apple: [-0.20842, -0.019668, 0.063981, -0.71403, -0.21181]\n",
            "pretrained vectors (159, 300) unk 0\n",
            "dictionary change 158 to 159 159\n",
            "\n",
            "### processing /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/data/bAbI/en\n",
            "data size 1000\n",
            "max sentnum 10\n",
            "max slen 72\n",
            "max qlen 6\n",
            "\n",
            "[('path', '/content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch'),\n",
            " ('data_dir',\n",
            "  '/content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/data/bAbI/en'),\n",
            " ('word2vec_type', 6),\n",
            " ('word2vec_path',\n",
            "  '/content/gdrive/My Drive/Colab '\n",
            "  'Notebooks/DMN_bAbI_pytorch/data/glove/glove.6B.300d.txt'),\n",
            " ('word_embed_dim', 300),\n",
            " ('batch_size', 32),\n",
            " ('max_sentnum',\n",
            "  {1: 10,\n",
            "   2: 88,\n",
            "   3: 320,\n",
            "   4: 2,\n",
            "   5: 126,\n",
            "   6: 26,\n",
            "   7: 52,\n",
            "   8: 58,\n",
            "   9: 10,\n",
            "   10: 10,\n",
            "   11: 10,\n",
            "   12: 10,\n",
            "   13: 10,\n",
            "   14: 14,\n",
            "   15: 8,\n",
            "   16: 9,\n",
            "   17: 2,\n",
            "   18: 20,\n",
            "   19: 5,\n",
            "   20: 12}),\n",
            " ('max_slen',\n",
            "  {1: 68,\n",
            "   2: 552,\n",
            "   3: 1875,\n",
            "   4: 16,\n",
            "   5: 782,\n",
            "   6: 156,\n",
            "   7: 321,\n",
            "   8: 358,\n",
            "   9: 74,\n",
            "   10: 89,\n",
            "   11: 76,\n",
            "   12: 87,\n",
            "   13: 86,\n",
            "   14: 116,\n",
            "   15: 44,\n",
            "   16: 41,\n",
            "   17: 24,\n",
            "   18: 169,\n",
            "   19: 40,\n",
            "   20: 69}),\n",
            " ('max_qlen',\n",
            "  {1: 4,\n",
            "   2: 5,\n",
            "   3: 8,\n",
            "   4: 7,\n",
            "   5: 8,\n",
            "   6: 6,\n",
            "   7: 7,\n",
            "   8: 5,\n",
            "   9: 6,\n",
            "   10: 6,\n",
            "   11: 4,\n",
            "   12: 4,\n",
            "   13: 4,\n",
            "   14: 7,\n",
            "   15: 6,\n",
            "   16: 5,\n",
            "   17: 12,\n",
            "   18: 10,\n",
            "   19: 11,\n",
            "   20: 8}),\n",
            " ('max_episode', 5),\n",
            " ('word_vocab_size', 159),\n",
            " ('save_preprocess', True),\n",
            " ('preprocess_save_path',\n",
            "  '/content/gdrive/My Drive/Colab '\n",
            "  'Notebooks/DMN_bAbI_pytorch/data/bAbI/babi(tmp).pkl'),\n",
            " ('preprocess_load_path',\n",
            "  '/content/gdrive/My Drive/Colab '\n",
            "  'Notebooks/DMN_bAbI_pytorch/data/bAbI/babi(10k).pkl')]\n",
            "\n",
            "100 100\n",
            "200 100\n",
            "300 100\n",
            "400 100\n",
            "500 100\n",
            "600 100\n",
            "700 100\n",
            "800 100\n",
            "900 100\n",
            "0 100\n",
            "[35, 25, 13, 4, 23, 6, 40, 25, 13, 4, 8, 6, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [33, 2, 35, 10] [23] [1, 11, 11, 11, 11] [6, 12, 0, 0, 0, 0, 0, 0, 0, 0] 4 2\n",
            "[6, 12]\n",
            "story: sandra moved to the office . daniel moved to the kitchen .\n",
            "question: where is sandra ?\n",
            "answer: ['office']\n",
            "supporting fact: [1, 11, 11, 11, 11]\n",
            "length of sentences: [6, 12]\n",
            "iteration test pass! te\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1crIKh97IO7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Args:\n",
        "  def __init__(self):\n",
        "    self.path = '/content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch'\n",
        "    self.data_path = self.path + '/data/bAbI/babi(tmp).pkl'\n",
        "    self.model_name = 'm'\n",
        "    self.checkpoint_dir = self.path + '/results/'\n",
        "    self.batch_size = 32\n",
        "    self.epoch = 100\n",
        "    self.train = 1\n",
        "    self.valid = 1\n",
        "    self.test = 1\n",
        "    self.early_stop = 0\n",
        "    self.resume = False\n",
        "    self.save = False\n",
        "    self.print_step = 128\n",
        "\n",
        "    # model hyperparameters\n",
        "    self.lr = 0.0003\n",
        "    self.lr_decay = 1.0\n",
        "    self.wd = 0\n",
        "    self.grad_max_norm = 5\n",
        "    self.s_rnn_hdim = 100\n",
        "    self.s_rnn_ln = 1\n",
        "    self.s_rnn_dr = 0.0\n",
        "    self.q_rnn_hdim = 100\n",
        "    self.q_rnn_ln = 1\n",
        "    self.q_rnn_dr = 0.0\n",
        "    self.e_cell_hdim = 100\n",
        "    self.m_cell_hdim = 100\n",
        "    self.a_cell_hdim = 100\n",
        "    self.word_dr = 0.2\n",
        "    self.g1_dim = 500\n",
        "    self.max_episode = 10\n",
        "    self.beta_cnt = 10\n",
        "    self.set_num = 1 # change this to cover all the bAbI tasks from 1 to 20\n",
        "    self.max_alen = 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LJUEQmoJnO9Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "args = Args()\n",
        "dataset = pickle.load(open(args.data_path,'rb'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMQwITOgMfA8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a28ccfff-f454-418c-9672-2cf12c33d85b"
      },
      "source": [
        "# update args\n",
        "dataset.config.__dict__.update(args.__dict__)\n",
        "args.__dict__.update(dataset.config.__dict__)\n",
        "pp = lambda x: pprint.PrettyPrinter().pprint(x)\n",
        "pp(args.__dict__)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'a_cell_hdim': 100,\n",
            " 'batch_size': 32,\n",
            " 'beta_cnt': 10,\n",
            " 'checkpoint_dir': '/content/gdrive/My Drive/Colab '\n",
            "                   'Notebooks/DMN_bAbI_pytorch/results/',\n",
            " 'data_dir': '/content/gdrive/My Drive/Colab '\n",
            "             'Notebooks/DMN_bAbI_pytorch/data/bAbI/en',\n",
            " 'data_path': '/content/gdrive/My Drive/Colab '\n",
            "              'Notebooks/DMN_bAbI_pytorch/data/bAbI/babi(tmp).pkl',\n",
            " 'e_cell_hdim': 100,\n",
            " 'early_stop': 0,\n",
            " 'epoch': 100,\n",
            " 'g1_dim': 500,\n",
            " 'grad_max_norm': 5,\n",
            " 'lr': 0.0003,\n",
            " 'lr_decay': 1.0,\n",
            " 'm_cell_hdim': 100,\n",
            " 'max_alen': 2,\n",
            " 'max_episode': 10,\n",
            " 'max_qlen': {1: 4,\n",
            "              2: 5,\n",
            "              3: 8,\n",
            "              4: 7,\n",
            "              5: 8,\n",
            "              6: 6,\n",
            "              7: 7,\n",
            "              8: 5,\n",
            "              9: 6,\n",
            "              10: 6,\n",
            "              11: 4,\n",
            "              12: 4,\n",
            "              13: 4,\n",
            "              14: 7,\n",
            "              15: 6,\n",
            "              16: 5,\n",
            "              17: 12,\n",
            "              18: 10,\n",
            "              19: 11,\n",
            "              20: 8},\n",
            " 'max_sentnum': {1: 10,\n",
            "                 2: 88,\n",
            "                 3: 320,\n",
            "                 4: 2,\n",
            "                 5: 126,\n",
            "                 6: 26,\n",
            "                 7: 52,\n",
            "                 8: 58,\n",
            "                 9: 10,\n",
            "                 10: 10,\n",
            "                 11: 10,\n",
            "                 12: 10,\n",
            "                 13: 10,\n",
            "                 14: 14,\n",
            "                 15: 8,\n",
            "                 16: 9,\n",
            "                 17: 2,\n",
            "                 18: 20,\n",
            "                 19: 5,\n",
            "                 20: 12},\n",
            " 'max_slen': {1: 68,\n",
            "              2: 552,\n",
            "              3: 1875,\n",
            "              4: 16,\n",
            "              5: 782,\n",
            "              6: 156,\n",
            "              7: 321,\n",
            "              8: 358,\n",
            "              9: 74,\n",
            "              10: 89,\n",
            "              11: 76,\n",
            "              12: 87,\n",
            "              13: 86,\n",
            "              14: 116,\n",
            "              15: 44,\n",
            "              16: 41,\n",
            "              17: 24,\n",
            "              18: 169,\n",
            "              19: 40,\n",
            "              20: 69},\n",
            " 'model_name': 'm',\n",
            " 'path': '/content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch',\n",
            " 'preprocess_load_path': '/content/gdrive/My Drive/Colab '\n",
            "                         'Notebooks/DMN_bAbI_pytorch/data/bAbI/babi(10k).pkl',\n",
            " 'preprocess_save_path': '/content/gdrive/My Drive/Colab '\n",
            "                         'Notebooks/DMN_bAbI_pytorch/data/bAbI/babi(tmp).pkl',\n",
            " 'print_step': 128,\n",
            " 'q_rnn_dr': 0.0,\n",
            " 'q_rnn_hdim': 100,\n",
            " 'q_rnn_ln': 1,\n",
            " 'resume': False,\n",
            " 's_rnn_dr': 0.0,\n",
            " 's_rnn_hdim': 100,\n",
            " 's_rnn_ln': 1,\n",
            " 'save': False,\n",
            " 'save_preprocess': True,\n",
            " 'set_num': 1,\n",
            " 'test': 1,\n",
            " 'train': 1,\n",
            " 'valid': 1,\n",
            " 'wd': 0,\n",
            " 'word2vec_path': '/content/gdrive/My Drive/Colab '\n",
            "                  'Notebooks/DMN_bAbI_pytorch/data/glove/glove.6B.300d.txt',\n",
            " 'word2vec_type': 6,\n",
            " 'word_dr': 0.2,\n",
            " 'word_embed_dim': 300,\n",
            " 'word_vocab_size': 159}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXARq8l3OhVA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def progress(_progress):\n",
        "    bar_length = 5  # Modify this to change the length of the progress bar\n",
        "    status = \"\"\n",
        "    if isinstance(_progress, int):\n",
        "        _progress = float(_progress)\n",
        "    if not isinstance(_progress, float):\n",
        "        _progress = 0\n",
        "        status = \"error: progress var must be float\\r\\n\"\n",
        "    if _progress < 0:\n",
        "        _progress = 0\n",
        "        status = \"Halt...\\r\\n\"\n",
        "    if _progress >= 1:\n",
        "        _progress = 1\n",
        "        status = \"\"\n",
        "    block = int(round(bar_length * _progress))\n",
        "    text = \"\\r\\t[%s]\\t%.2f%% %s\" % (\n",
        "            \"#\" * block + \" \" * (bar_length-block), _progress * 100, status)\n",
        "\n",
        "    return text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcX6Chl9OO16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_epoch(m, d, ep, mode='tr', set_num=1, is_train=True):\n",
        "    total_metrics = np.zeros(2)\n",
        "    total_step = 0.0\n",
        "    print_step = m.config.print_step\n",
        "    start_time = datetime.now()\n",
        "    d.shuffle_data(seed=None, mode='tr')\n",
        "\n",
        "    while True:\n",
        "        m.optimizer.zero_grad()\n",
        "        stories, questions, answers, sup_facts, s_lens, q_lens, e_lens= \\\n",
        "                d.get_next_batch(mode, set_num)\n",
        "        #d.decode_data(stories[0], questions[0], answers[0], sup_facts[0], s_lens[0])\n",
        "        wrap_tensor = lambda x: torch.LongTensor(np.array(x))\n",
        "        wrap_var = lambda x: Variable(wrap_tensor(x)).cuda()\n",
        "        stories = wrap_var(stories)\n",
        "        questions = wrap_var(questions)\n",
        "        answers = wrap_var(answers)\n",
        "        sup_facts = wrap_var(sup_facts) - 1\n",
        "        s_lens = wrap_tensor(s_lens)\n",
        "        q_lens = wrap_tensor(q_lens)\n",
        "        e_lens = wrap_tensor(e_lens)\n",
        "\n",
        "        if is_train: m.train()\n",
        "        else: m.eval()\n",
        "        outputs, gates = m(stories, questions, s_lens, q_lens, e_lens)\n",
        "        a_loss = m.criterion(outputs[:,0,:], answers[:,0])\n",
        "        if answers.size(1) > 1: # multiple answer\n",
        "            for ans_idx in range(m.config.max_alen):\n",
        "                a_loss += m.criterion(outputs[:,ans_idx,:], answers[:,ans_idx])\n",
        "        for episode in range(5):\n",
        "            if episode == 0:\n",
        "                g_loss = m.criterion(gates[:,episode,:], sup_facts[:,episode]) \n",
        "            else:\n",
        "                g_loss += m.criterion(gates[:,episode,:], sup_facts[:,episode])\n",
        "        beta = 0 if ep < m.config.beta_cnt and mode == 'tr' else 1\n",
        "        alpha = 1\n",
        "        metrics = m.get_metrics(outputs, answers, multiple=answers.size(1)>1)\n",
        "        total_loss = alpha * g_loss + beta * a_loss\n",
        "\n",
        "        if is_train:\n",
        "            total_loss.backward()\n",
        "            nn.utils.clip_grad_norm(m.parameters(), m.config.grad_max_norm)\n",
        "            m.optimizer.step()\n",
        "\n",
        "        total_metrics[0] += total_loss.data\n",
        "        total_metrics[1] += metrics\n",
        "        total_step += 1.0\n",
        "        \n",
        "        # print step\n",
        "        if d.get_batch_ptr(mode) % print_step == 0 or total_step == 1:\n",
        "            et = int((datetime.now() - start_time).total_seconds())\n",
        "            _progress = progress(\n",
        "                    d.get_batch_ptr(mode) / d.get_dataset_len(mode, set_num))\n",
        "            if d.get_batch_ptr(mode) == 0:\n",
        "                _progress = progress(1)\n",
        "            _progress += '[%s] time: %s' % (\n",
        "                    '\\t'.join(['{:.2f}'.format(k) \n",
        "                    for k in total_metrics / total_step]),\n",
        "                    '{:2d}:{:2d}:{:2d}'.format(et//3600, et%3600//60, et%60))\n",
        "            sys.stdout.write(_progress)\n",
        "            sys.stdout.flush()\n",
        "\n",
        "            # end of an epoch\n",
        "            if d.get_batch_ptr(mode) == 0:\n",
        "                et = (datetime.now() - start_time).total_seconds()\n",
        "                print('\\n\\ttotal metrics:\\t%s' % ('\\t'.join(['{:.2f}'.format(k)\n",
        "                    for k in total_metrics / total_step]))) \n",
        "                break\n",
        "\n",
        "    return total_metrics / total_step"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIFRQUtnMouS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DMN(nn.Module):\n",
        "    def __init__(self, config, idx2vec, set_num):\n",
        "        super(DMN, self).__init__()\n",
        "        self.config = config\n",
        "        self.set_num = set_num\n",
        "\n",
        "        # embedding layers\n",
        "        self.word_embed = nn.Embedding(config.word_vocab_size, config.word_embed_dim,\n",
        "                padding_idx=0)\n",
        "        \n",
        "        # dimensions according to settings\n",
        "        self.s_rnn_idim = config.word_embed_dim\n",
        "        self.q_rnn_idim = config.word_embed_dim\n",
        "        self.e_cell_idim = config.s_rnn_hdim\n",
        "        self.m_cell_idim = config.e_cell_hdim\n",
        "        self.a_cell_idim = config.q_rnn_hdim + config.word_vocab_size\n",
        "        # self.z_dim = config.s_rnn_hdim * 7 + 2\n",
        "        self.z_dim = config.s_rnn_hdim * 4\n",
        "\n",
        "        # rnn layers\n",
        "        self.s_rnn = nn.GRU(self.s_rnn_idim, config.s_rnn_hdim, batch_first=True)\n",
        "        self.q_rnn = nn.GRU(self.q_rnn_idim, config.q_rnn_hdim, batch_first=True)\n",
        "        self.e_cell = nn.GRUCell(self.e_cell_idim, config.e_cell_hdim)\n",
        "        self.m_cell = nn.GRUCell(self.m_cell_idim, config.m_cell_hdim)\n",
        "        self.a_cell = nn.GRUCell(self.a_cell_idim, config.a_cell_hdim)\n",
        "\n",
        "        # linear layers\n",
        "        # self.z_sq = nn.Linear(config.s_rnn_hdim, config.q_rnn_hdim, bias=False)\n",
        "        # self.z_sm = nn.Linear(config.s_rnn_hdim, config.m_cell_hdim, bias=False)\n",
        "        self.out = nn.Linear(config.m_cell_hdim, \n",
        "                config.word_vocab_size, bias=False)\n",
        "        self.g1 = nn.Linear(self.z_dim, config.g1_dim)\n",
        "        self.g2 = nn.Linear(config.g1_dim, 1)\n",
        "\n",
        "        # initialization\n",
        "        self.init_word_embed(idx2vec)\n",
        "        params = self.model_params(debug=False)\n",
        "        self.optimizer = optim.Adam(params, lr=config.lr)\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def init_word_embed(self, idx2vec):\n",
        "        self.word_embed.weight.data.copy_(torch.from_numpy(np.array(idx2vec)))\n",
        "        self.word_embed.weight.requires_grad = False\n",
        "\n",
        "    def model_params(self, debug=True):\n",
        "        print('model parameters: ', end='')\n",
        "        params = []\n",
        "        total_size = 0\n",
        "        def multiply_iter(p_list):\n",
        "            out = 1\n",
        "            for p in p_list:\n",
        "                out *= p\n",
        "            return out\n",
        "\n",
        "        for p in self.parameters():\n",
        "            if p.requires_grad:\n",
        "                params.append(p)\n",
        "                total_size += multiply_iter(p.size())\n",
        "            if debug:\n",
        "                print(p.requires_grad, p.size())\n",
        "        print('%s\\n' % '{:,}'.format(total_size))\n",
        "        return params\n",
        "    \n",
        "    def init_rnn_h(self, batch_size):\n",
        "        return Variable(torch.zeros(\n",
        "            self.config.s_rnn_ln*1, batch_size, self.config.s_rnn_hdim)).cuda()\n",
        "\n",
        "    def init_cell_h(self, batch_size):\n",
        "        return Variable(torch.zeros(batch_size, self.config.s_rnn_hdim)).cuda()\n",
        "\n",
        "    def input_module(self, stories, s_lens):\n",
        "        word_embed = F.dropout(self.word_embed(stories), self.config.word_dr)\n",
        "        init_s_rnn_h = self.init_rnn_h(stories.size(0))\n",
        "        gru_out, _ = self.s_rnn(word_embed, init_s_rnn_h)\n",
        "        gru_out = gru_out.contiguous().view(-1, self.config.s_rnn_hdim).cpu()\n",
        "        s_lens_offset = (torch.arange(0, stories.size(0)).type(torch.LongTensor)\n",
        "                * self.config.max_slen[self.set_num]).unsqueeze(1)\n",
        "        s_lens = (torch.clamp(s_lens + s_lens_offset - 1, min=0)).view(-1)\n",
        "        selected = gru_out[s_lens,:].view(-1, self.config.max_sentnum[self.set_num],\n",
        "                self.config.s_rnn_hdim).cuda()\n",
        "        return selected \n",
        "\n",
        "    def question_module(self, questions, q_lens):\n",
        "        word_embed = F.dropout(self.word_embed(questions), self.config.word_dr)\n",
        "        init_q_rnn_h = self.init_rnn_h(questions.size(0))\n",
        "        gru_out, _ = self.q_rnn(word_embed, init_q_rnn_h)\n",
        "        gru_out = gru_out.contiguous().view(-1, self.config.q_rnn_hdim).cpu()\n",
        "        q_lens = (torch.arange(0, questions.size(0)).type(torch.LongTensor)\n",
        "                * self.config.max_qlen[self.set_num] + q_lens - 1)\n",
        "        selected = gru_out[q_lens,:].view(-1, self.config.q_rnn_hdim).cuda() \n",
        "\n",
        "        return selected\n",
        "\n",
        "    def episodic_memory_module(self, s_rep, q_rep, e_lens, memory):\n",
        "        # expand s_rep to have sentinel\n",
        "        sentinel = Variable(torch.zeros(\n",
        "            s_rep.size(0), 1, self.config.s_rnn_hdim)).cuda()\n",
        "        s_rep = torch.cat((s_rep, sentinel), 1)\n",
        "        q_rep = q_rep.unsqueeze(1).expand_as(s_rep)\n",
        "        memory = memory.unsqueeze(1).expand_as(s_rep)\n",
        "        # sw = self.z_sq(s_rep.view(-1, self.config.s_rnn_hdim)).view(\n",
        "        #         q_rep.size())\n",
        "        # swq = torch.sum(sw * q_rep, 2, keepdim=True)\n",
        "        # swm = torch.sum(sw * memory, 2, keepdim=True)\n",
        "        # Z = torch.cat([s_rep, memory, q_rep, s_rep*q_rep, s_rep*memory,\n",
        "        #     torch.abs(s_rep-q_rep), torch.abs(s_rep-memory), swq, swm], 2)\n",
        "        Z = torch.cat([s_rep*q_rep, s_rep*memory,\n",
        "            torch.abs(s_rep-q_rep), torch.abs(s_rep-memory)], 2)\n",
        "        G = self.g2(F.tanh(self.g1(Z.view(-1, self.z_dim))))\n",
        "        G_s = F.sigmoid(G).view(\n",
        "                -1, self.config.max_sentnum[self.set_num] + 1).unsqueeze(2)\n",
        "        G_s = torch.transpose(G_s, 0, 1).contiguous()\n",
        "        s_rep = torch.transpose(s_rep, 0, 1).contiguous()\n",
        "        # print('g', G.size())\n",
        "\n",
        "        e_rnn_h = self.init_cell_h(s_rep.size(1))\n",
        "        # print('input', s_rep.size())\n",
        "        # print('hidden', e_rnn_h.size())\n",
        "        hiddens = []\n",
        "        for step, (gg, ss) in enumerate(zip(G_s, s_rep)):\n",
        "            e_rnn_h = gg * self.e_cell(ss, e_rnn_h) + (1 - gg) * e_rnn_h\n",
        "            hiddens.append(e_rnn_h)\n",
        "        hiddens = torch.transpose(torch.stack(hiddens), 0, 1).contiguous().view(\n",
        "                -1, self.config.e_cell_hdim).cpu()\n",
        "        e_lens = (torch.arange(0, s_rep.size(1)).type(torch.LongTensor)\n",
        "                * (self.config.max_sentnum[self.set_num]+1) + e_lens - 1)\n",
        "        selected = hiddens[e_lens,:].view(-1, self.config.e_cell_hdim).cuda() \n",
        "        # print('out', selected.size())\n",
        "        return selected, G.view(-1, self.config.max_sentnum[self.set_num] + 1)\n",
        "\n",
        "    def answer_module(self, q_rep, memory):\n",
        "        y = F.softmax(self.out(memory))\n",
        "        a_rnn_h = memory\n",
        "        ys = []\n",
        "        #print('q_rep', q_rep[0,:])\n",
        "        for step in range(self.config.max_alen):\n",
        "            a_rnn_h = self.a_cell(torch.cat((y, q_rep), 1), a_rnn_h)\n",
        "            z = self.out(a_rnn_h)\n",
        "            y = F.softmax(z)\n",
        "            ys.append(z)\n",
        "        ys = torch.transpose(torch.stack(ys), 0, 1).contiguous()\n",
        "        \"\"\"\n",
        "        z = self.out(torch.cat((memory, q_rep), 1))\n",
        "        ys = torch.transpose(torch.stack([z]), 0, 1).contiguous()\n",
        "        \"\"\"\n",
        "        return ys\n",
        "\n",
        "    def forward(self, stories, questions, s_lens, q_lens, e_lens):\n",
        "        s_rep = self.input_module(stories, s_lens)\n",
        "        q_rep = self.question_module(questions, q_lens)\n",
        "        # print('stories', s_rep.size())\n",
        "        # print('questions', q_rep.size())\n",
        "        \n",
        "        memory = q_rep # initial memory\n",
        "        gates = []\n",
        "        for episode in range(self.config.max_episode):\n",
        "            e_rep, gate = self.episodic_memory_module(s_rep, q_rep, e_lens, memory)\n",
        "            gates.append(gate)\n",
        "            memory = self.m_cell(e_rep, memory)\n",
        "        gates = torch.transpose(torch.stack(gates), 0, 1).contiguous()\n",
        "        outputs = self.answer_module(q_rep, memory)\n",
        "        # print('memory', memory.size())\n",
        "        # print('outputs', outputs.size())\n",
        "\n",
        "        return outputs, gates\n",
        "\n",
        "    def get_regloss(self, weight_decay=None):\n",
        "        if weight_decay is None:\n",
        "            weight_decay = self.config.wd\n",
        "        reg_loss = 0\n",
        "        params = [] # add params here\n",
        "        for param in params:\n",
        "            reg_loss += torch.norm(param.weight, 2)\n",
        "        return reg_loss * weight_decay\n",
        "\n",
        "    def decay_lr(self, lr_decay=None):\n",
        "        if lr_decay is None:\n",
        "            lr_decay = self.config.lr_decay\n",
        "        self.config.lr /= lr_decay\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.config.lr\n",
        "\n",
        "        print('\\tlearning rate decay to %.3f' % self.config.lr)\n",
        "\n",
        "    def get_metrics(self, outputs, targets, multiple=False):\n",
        "        if not multiple:\n",
        "            outputs = outputs[:,0,:]\n",
        "            targets = targets[:,0]\n",
        "\n",
        "            max_idx = torch.max(outputs, 1)[1].data.cpu().numpy()\n",
        "            outputs_topk = torch.topk(outputs, 3)[1].data.cpu().numpy()\n",
        "            targets = targets.data.cpu().numpy()\n",
        "\n",
        "            acc = np.mean([float(k == tk[0]) for (k, tk)\n",
        "                in zip(targets, outputs_topk)]) * 100\n",
        "        else:\n",
        "            topk_list = []\n",
        "            target_list = []\n",
        "            o_outputs = outputs[:]\n",
        "            o_targets = targets[:]\n",
        "            for idx in range(outputs.size(1)):\n",
        "                outputs = o_outputs[:,idx,:]\n",
        "                targets = o_targets[:,idx]\n",
        "                max_idx = torch.max(outputs, 1)[1].data.cpu().numpy()\n",
        "                outputs_topk = torch.topk(outputs, 3)[1].data.cpu().numpy()\n",
        "                targets = targets.data.cpu().numpy()\n",
        "                \n",
        "                topk_list.append(outputs_topk)\n",
        "                target_list.append(targets)\n",
        "\n",
        "            acc = np.array([1.0 for _ in range(outputs.size(0))])\n",
        "            for target, topk in zip(target_list, topk_list):\n",
        "                acc *= np.array([float(k == tk[0] or k == -100) \\\n",
        "                        for (k, tk) in zip(target, topk)])\n",
        "                # print(acc)\n",
        "            acc = np.mean(acc) * 100\n",
        "\n",
        "        return acc\n",
        " \n",
        "    def save_checkpoint(self, state, filename=None):\n",
        "        if filename is None:\n",
        "            filename = (self.config.checkpoint_dir +\\\n",
        "                    self.config.model_name + str(self.set_num) + '.pth')\n",
        "        else:\n",
        "            filename = self.config.checkpoint_dir + filename\n",
        "        print('\\t=> save checkpoint %s' % filename)\n",
        "        torch.save(state, filename)\n",
        "\n",
        "    def load_checkpoint(self, filename=None):\n",
        "        if filename is None:\n",
        "            filename = (self.config.checkpoint_dir +\\\n",
        "                    self.config.model_name + str(self.set_num) + '.pth')\n",
        "        else:\n",
        "            filename = self.config.checkpoint_dir + filename\n",
        "        print('\\t=> load checkpoint %s' % filename)\n",
        "        checkpoint = torch.load(filename)\n",
        "        self.load_state_dict(checkpoint['state_dict'])\n",
        "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "        # self.config = checkpoint['config']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oo7-67HeNen-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_experiment(model, dataset, set_num):\n",
        "    best_metric = np.zeros(2)\n",
        "    early_stop = False\n",
        "    if model.config.train:\n",
        "        if model.config.resume:\n",
        "            model.load_checkpoint()\n",
        "\n",
        "        for ep in range(model.config.epoch):\n",
        "            if early_stop:\n",
        "                break\n",
        "            print('- Training Epoch %d' % (ep+1))\n",
        "            run_epoch(model, dataset, ep, 'tr', set_num)\n",
        "\n",
        "            if model.config.valid:\n",
        "                print('- Validation')\n",
        "                met = run_epoch(model, dataset, ep, 'va', set_num, False)\n",
        "                if best_metric[1] < met[1]:\n",
        "                    best_metric = met\n",
        "                    model.save_checkpoint({\n",
        "                        'config': model.config,\n",
        "                        'state_dict': model.state_dict(),\n",
        "                        'optimizer': model.optimizer.state_dict()})\n",
        "                    if best_metric[1] == 100:\n",
        "                        break\n",
        "                else:\n",
        "                    # model.decay_lr()\n",
        "                    if model.config.early_stop:\n",
        "                        early_stop = True\n",
        "                        print('\\tearly stop applied')\n",
        "                print('\\tbest metrics:\\t%s' % ('\\t'.join(['{:.2f}'.format(k)\n",
        "                    for k in best_metric])))\n",
        "\n",
        "            if model.config.test:\n",
        "                print('- Testing')\n",
        "                run_epoch(model, dataset, ep, 'te', set_num, False)\n",
        "            print()\n",
        "    \n",
        "    if model.config.test:\n",
        "        print('- Load Validation/Testing')\n",
        "        if model.config.resume or model.config.train:\n",
        "            model.load_checkpoint()\n",
        "        run_epoch(model, dataset, 0, 'va', set_num, False)\n",
        "        run_epoch(model, dataset, 0, 'te', set_num, False)\n",
        "        print()\n",
        "\n",
        "    return best_metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZH69DdatNki5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c38ae98-ddb0-43e2-8493-10d45540c150"
      },
      "source": [
        "# new model experiment\n",
        "    for set_num in range(args.set_num, args.set_num+1):\n",
        "        print('\\n[QA set %d]' % (set_num))\n",
        "        model = DMN(args, dataset.idx2vec, set_num).cuda()\n",
        "        results = run_experiment(model, dataset, set_num)\n",
        "\n",
        "    print('### end of experiment')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[QA set 1]\n",
            "model parameters: 687,601\n",
            "\n",
            "- Training Epoch 1\n",
            "\r\t[     ]\t0.71% [11.95\t0.00] time:  0: 0: 0"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:132: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:139: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:41: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\t[#####]\t100.00% [1.76\t0.00] time:  0: 0:27\n",
            "\ttotal metrics:\t1.76\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.23\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.23\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.22\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.22\t0.00\n",
            "\n",
            "- Training Epoch 2\n",
            "\t[#####]\t100.00% [0.04\t0.00] time:  0: 0:27\n",
            "\ttotal metrics:\t0.04\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.12\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.12\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.11\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.11\t0.00\n",
            "\n",
            "- Training Epoch 3\n",
            "\t[#####]\t100.00% [0.01\t0.00] time:  0: 0:27\n",
            "\ttotal metrics:\t0.01\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.10\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.10\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.10\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.10\t0.00\n",
            "\n",
            "- Training Epoch 4\n",
            "\t[#####]\t100.00% [0.01\t0.00] time:  0: 0:27\n",
            "\ttotal metrics:\t0.01\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.10\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.10\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.10\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.10\t0.00\n",
            "\n",
            "- Training Epoch 5\n",
            "\t[#####]\t100.00% [0.01\t0.00] time:  0: 0:27\n",
            "\ttotal metrics:\t0.01\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\n",
            "- Training Epoch 6\n",
            "\t[#####]\t100.00% [0.00\t0.00] time:  0: 0:27\n",
            "\ttotal metrics:\t0.00\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\n",
            "- Training Epoch 7\n",
            "\t[#####]\t100.00% [0.00\t0.00] time:  0: 0:27\n",
            "\ttotal metrics:\t0.00\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\n",
            "- Training Epoch 8\n",
            "\t[#####]\t100.00% [0.00\t0.00] time:  0: 0:26\n",
            "\ttotal metrics:\t0.00\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\n",
            "- Training Epoch 9\n",
            "\t[#####]\t100.00% [0.00\t0.00] time:  0: 0:26\n",
            "\ttotal metrics:\t0.00\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\n",
            "- Training Epoch 10\n",
            "\t[#####]\t100.00% [0.00\t0.00] time:  0: 0:26\n",
            "\ttotal metrics:\t0.00\t0.00\n",
            "- Validation\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\tbest metrics:\t0.00\t0.00\n",
            "- Testing\n",
            "\t[#####]\t100.00% [5.09\t0.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t5.09\t0.00\n",
            "\n",
            "- Training Epoch 11\n",
            "\t[#####]\t100.00% [1.92\t15.94] time:  0: 0:26\n",
            "\ttotal metrics:\t1.92\t15.94\n",
            "- Validation\n",
            "\t[#####]\t100.00% [1.80\t16.70] time:  0: 0: 1\n",
            "\ttotal metrics:\t1.80\t16.70\n",
            "\t=> save checkpoint /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/results/m1.pth\n",
            "\tbest metrics:\t1.80\t16.70\n",
            "- Testing\n",
            "\t[#####]\t100.00% [1.80\t16.99] time:  0: 0: 1\n",
            "\ttotal metrics:\t1.80\t16.99\n",
            "\n",
            "- Training Epoch 12\n",
            "\t[#####]\t100.00% [1.80\t16.78] time:  0: 0:26\n",
            "\ttotal metrics:\t1.80\t16.78\n",
            "- Validation\n",
            "\t[#####]\t100.00% [1.80\t17.19] time:  0: 0: 1\n",
            "\ttotal metrics:\t1.80\t17.19\n",
            "\t=> save checkpoint /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/results/m1.pth\n",
            "\tbest metrics:\t1.80\t17.19\n",
            "- Testing\n",
            "\t[#####]\t100.00% [1.81\t15.72] time:  0: 0: 1\n",
            "\ttotal metrics:\t1.81\t15.72\n",
            "\n",
            "- Training Epoch 13\n",
            "\t[#####]\t100.00% [1.80\t16.51] time:  0: 0:26\n",
            "\ttotal metrics:\t1.80\t16.51\n",
            "- Validation\n",
            "\t[#####]\t100.00% [1.80\t14.26] time:  0: 0: 1\n",
            "\ttotal metrics:\t1.80\t14.26\n",
            "\tbest metrics:\t1.80\t17.19\n",
            "- Testing\n",
            "\t[#####]\t100.00% [1.80\t15.33] time:  0: 0: 1\n",
            "\ttotal metrics:\t1.80\t15.33\n",
            "\n",
            "- Training Epoch 14\n",
            "\t[#####]\t100.00% [1.23\t54.24] time:  0: 0:26\n",
            "\ttotal metrics:\t1.23\t54.24\n",
            "- Validation\n",
            "\t[#####]\t100.00% [0.45\t80.57] time:  0: 0: 1\n",
            "\ttotal metrics:\t0.45\t80.57\n",
            "\t=> save checkpoint /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/results/m1.pth\n",
            "\tbest metrics:\t0.45\t80.57\n",
            "- Testing\n",
            "\t[#####]\t100.00% [0.44\t81.45] time:  0: 0: 1\n",
            "\ttotal metrics:\t0.44\t81.45\n",
            "\n",
            "- Training Epoch 15\n",
            "\t[#####]\t100.00% [0.14\t98.42] time:  0: 0:26\n",
            "\ttotal metrics:\t0.14\t98.42\n",
            "- Validation\n",
            "\t[#####]\t100.00% [0.04\t100.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t0.04\t100.00\n",
            "\t=> save checkpoint /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/results/m1.pth\n",
            "- Load Validation/Testing\n",
            "\t=> load checkpoint /content/gdrive/My Drive/Colab Notebooks/DMN_bAbI_pytorch/results/m1.pth\n",
            "\t[#####]\t100.00% [0.04\t100.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t0.04\t100.00\n",
            "\t[#####]\t100.00% [0.04\t100.00] time:  0: 0: 1\n",
            "\ttotal metrics:\t0.04\t100.00\n",
            "\n",
            "### end of experiment\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}